wandb_version: 1

amp_scaler:
  desc: null
  value: true
anneal_epochs:
  desc: null
  value: 1
anneal_strategy:
  desc: null
  value: cos
attention_probs_dropout_prob:
  desc: null
  value: 0.1
awp:
  desc: null
  value: false
awp_eps:
  desc: null
  value: 0.01
awp_lr:
  desc: null
  value: 0.0001
batch_scheduler:
  desc: null
  value: true
batch_size:
  desc: null
  value: 8
betas:
  desc: null
  value:
  - 0.9
  - 0.999
cfg_name:
  desc: null
  value: CFG
checkpoint_dir:
  desc: null
  value: ./checkpoint/model/k15_type2_fold4/
clipping_grad:
  desc: null
  value: true
competition:
  desc: null
  value: CommonLit
dataset:
  desc: null
  value: OneToManyDataset
device:
  desc: null
  value: cuda:0
epochs:
  desc: null
  value: 30
freeze:
  desc: null
  value: false
gpu_id:
  desc: null
  value: 0
gradient_checkpoint:
  desc: null
  value: true
hidden_dropout_prob:
  desc: null
  value: 0.1
init_weight:
  desc: null
  value: kaiming_uniform
layerwise_adam_epsilon:
  desc: null
  value: 1.0e-08
layerwise_lr:
  desc: null
  value: 7.0e-05
layerwise_lr_decay:
  desc: null
  value: 0.9
layerwise_use_bertadam:
  desc: null
  value: false
layerwise_weight_decay:
  desc: null
  value: 0.01
llrd:
  desc: null
  value: true
load_pretrained:
  desc: null
  value: false
loop:
  desc: null
  value: train_loop
loss_fn:
  desc: null
  value: RMSELoss
max_grad_norm:
  desc: null
  value: 100
max_len:
  desc: null
  value: 2048
metrics:
  desc: null
  value: MCRMSELoss
model:
  desc: null
  value: microsoft/deberta-v3-large
model_arch:
  desc: null
  value: OneToManyModel
n_folds:
  desc: null
  value: 7
n_gpu:
  desc: null
  value: 1
n_gradient_accumulation_steps:
  desc: null
  value: 1
name:
  desc: null
  value: OneToManyTrainer
nth_awp_start_epoch:
  desc: null
  value: 2
num_cycles:
  desc: null
  value: 3
num_freeze:
  desc: null
  value: 4
num_reinit:
  desc: null
  value: 1
num_workers:
  desc: null
  value: 4
optimizer:
  desc: null
  value: AdamW
optuna:
  desc: null
  value: false
pooling:
  desc: null
  value: SubSequenceGEMPooling
reduction:
  desc: null
  value: none
reinit:
  desc: null
  value: true
resume:
  desc: null
  value: false
scheduler:
  desc: null
  value: cosine_annealing
seed:
  desc: null
  value: 42
smart_batch:
  desc: null
  value: 8
state_dict:
  desc: null
  value: ''
stop_mode:
  desc: null
  value: min
swa:
  desc: null
  value: false
swa_lr:
  desc: null
  value: 5.0e-06
swa_start:
  desc: null
  value: 135
test:
  desc: null
  value: false
tokenizer:
  desc: null
  value: 'DebertaV2TokenizerFast(name_or_path=''microsoft/deberta-v3-large'', vocab_size=128000,
    model_max_length=1000000000000000019884624838656, is_fast=True, padding_side=''right'',
    truncation_side=''right'', special_tokens={''bos_token'': ''[CLS]'', ''eos_token'':
    ''[SEP]'', ''unk_token'': ''[UNK]'', ''sep_token'': ''[SEP]'', ''pad_token'':
    ''[PAD]'', ''cls_token'': ''[CLS]'', ''mask_token'': ''[MASK]'', ''additional_special_tokens'':
    ['' [ANC] '']}, clean_up_tokenization_spaces=True)'
train:
  desc: null
  value: true
val_batch_size:
  desc: null
  value: 8
val_loss_fn:
  desc: null
  value: RMSELoss
wandb:
  desc: null
  value: true
warmup_ratio:
  desc: null
  value: 0.1
_wandb:
  desc: null
  value:
    python_version: 3.9.13
    cli_version: 0.15.8
    framework: huggingface
    huggingface_version: 4.31.0
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1695447949.239534
    t:
      1:
      - 1
      - 5
      - 11
      - 49
      - 53
      - 55
      2:
      - 1
      - 2
      - 3
      - 5
      - 11
      - 49
      - 53
      - 55
      3:
      - 13
      - 16
      - 23
      4: 3.9.13
      5: 0.15.8
      6: 4.31.0
      8:
      - 5
