{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Activate if you don't have those package, optuna, lightgbm \"\"\"\n",
    "!pip install optuna\n",
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import warnings\n",
    "import logging\n",
    "import os, sys, gc, random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.disable(logging.ERROR)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fe458b9183556a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" Helper Function \"\"\"\n",
    "def load_data(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\" Load data_folder from csv file like as train.csv, test.csv, val.csv \"\"\"\n",
    "    df = pd.read_csv(data_path)\n",
    "    return df\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    rmse = mean_squared_error(labels, predictions, squared=False)\n",
    "    return {\"rmse\": rmse}\n",
    "\n",
    "def compute_mcrmse(eval_pred):\n",
    "    \"\"\"\n",
    "    Calculates mean columnwise root mean squared error\n",
    "    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n",
    "    \"\"\"\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n",
    "    mcrmse = np.mean(col_rmse)\n",
    "\n",
    "    return {\n",
    "        \"content_rmse\": col_rmse[0],\n",
    "        \"wording_rmse\": col_rmse[1],\n",
    "        \"mcrmse\": mcrmse,\n",
    "    }\n",
    "\n",
    "def compt_score(content_true, content_pred, wording_true, wording_pred):\n",
    "    content_score = mean_squared_error(content_true, content_pred)**(1/2)\n",
    "    wording_score = mean_squared_error(wording_true, wording_pred)**(1/2)\n",
    "    \n",
    "    return (content_score + wording_score)/2\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "seed_everything(seed=42)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5231a98caf8c4b48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" Load Out of Fold Train Dataframe and Check Features of Dataframe \"\"\"\n",
    "\n",
    "train = load_data('oof_train 경로 입력')\n",
    "train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c1663959af673"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" Feature Selection for LightGBM, Optuna \"\"\"\n",
    "\n",
    "targets = [\"content\", \"wording\"]\n",
    "\n",
    "drop_columns = [\"fold\", \"student_id\", \"prompt_id\", \"text\", \"fixed_summary_text\",\n",
    "                \"prompt_question\", \"prompt_title\", \n",
    "                \"prompt_text\"] + targets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd8d95c7a8c782e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" Optuna Hyper-Params Tuning for LGBM \"\"\"\n",
    "\n",
    "def objective(trial, X_train_cv, y_train_cv, X_eval_cv, y_eval_cv):\n",
    "    dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n",
    "    dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 8)\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'random_state': 42,\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'max_depth': max_depth,\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 2**max_depth - 1),\n",
    "        'verbosity': -1  # Add this line to suppress warnings and info messages\n",
    "    }\n",
    "\n",
    "    evaluation_results = {}\n",
    "    model = lgb.train(params,\n",
    "                      num_boost_round=10000,\n",
    "                      valid_names=['train', 'valid'],\n",
    "                      train_set=dtrain,\n",
    "                      valid_sets=dval,\n",
    "                    #   verbose_eval=1000,\n",
    "                      callbacks=[\n",
    "                              lgb.early_stopping(stopping_rounds=30, verbose=True),\n",
    "                              lgb.log_evaluation(100),\n",
    "                              lgb.callback.record_evaluation(evaluation_results),\n",
    "                        ],)\n",
    "\n",
    "    # Use the last metric for early stopping\n",
    "    evals_result = model.best_score\n",
    "    last_metric = list(evals_result.values())[-1]\n",
    "    trial.set_user_attr('best_model', model)  # Save the model in the trial\n",
    "    return last_metric[list(last_metric.keys())[-1]]\n",
    "\n",
    "model_dict = {}\n",
    "\n",
    "for target in targets:\n",
    "    models = []\n",
    "    \n",
    "    for fold in range(4):\n",
    "        X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n",
    "        y_train_cv = train[train[\"fold\"] != fold][target]\n",
    "\n",
    "        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n",
    "        y_eval_cv = train[train[\"fold\"] == fold][target]\n",
    "\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: objective(trial, X_train_cv, y_train_cv, X_eval_cv, y_eval_cv), n_trials=100)\n",
    "        \n",
    "        print('Best trial: score {}, params {}'.format(study.best_value, study.best_params))\n",
    "        best_model = study.trials[study.best_trial.number].user_attrs['best_model']\n",
    "        models.append(best_model)\n",
    "    \n",
    "    model_dict[target] = models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ea462a05a7528c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" validation of oputna result \"\"\"\n",
    "\n",
    "\n",
    "model_dict = {}\n",
    "\n",
    "for target in targets:\n",
    "    models = []\n",
    "    \n",
    "    for fold in range(4):\n",
    "        X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n",
    "        y_train_cv = train[train[\"fold\"] != fold][target]\n",
    "\n",
    "        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n",
    "        y_eval_cv = train[train[\"fold\"] == fold][target]\n",
    "\n",
    "        dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n",
    "        dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n",
    "        \n",
    "        # input params from optuna result\n",
    "        params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'random_state': 42,\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'learning_rate': 0.036873986853722646,\n",
    "            'max_depth': 3,  #3\n",
    "            'lambda_l1': 0.00010854204935611854,\n",
    "            'lambda_l2': 2.8816343485123506e-05,\n",
    "            'num_leaves': 4\n",
    "        }\n",
    "\n",
    "        evaluation_results = {}\n",
    "        model = lgb.train(params,\n",
    "                          num_boost_round=10000,\n",
    "                            #categorical_feature = categorical_features,\n",
    "                          valid_names=['train', 'valid'],\n",
    "                          train_set=dtrain,\n",
    "                          valid_sets=dval,\n",
    "                          callbacks=[\n",
    "                              lgb.early_stopping(stopping_rounds=30, verbose=True),\n",
    "                              lgb.log_evaluation(100),\n",
    "                              lgb.callback.record_evaluation(evaluation_results)\n",
    "                          ],\n",
    "                         )\n",
    "        models.append(model)\n",
    "    \n",
    "    model_dict[target] = models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff31f4ce973406d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cv\n",
    "rmses = []\n",
    "\n",
    "for target in targets:\n",
    "    models = model_dict[target]\n",
    "\n",
    "    preds = []\n",
    "    trues = []\n",
    "    \n",
    "    for fold, model in enumerate(models):\n",
    "        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n",
    "        y_eval_cv = train[train[\"fold\"] == fold][target]\n",
    "\n",
    "        pred = model.predict(X_eval_cv)\n",
    "\n",
    "        trues.extend(y_eval_cv)\n",
    "        preds.extend(pred)\n",
    "        \n",
    "    rmse = np.sqrt(mean_squared_error(trues, preds))\n",
    "    print(f\"{target}_rmse : {rmse}\")\n",
    "    rmses = rmses + [rmse]\n",
    "\n",
    "print(f\"mcrmse : {sum(rmses) / len(rmses)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b2777b70006d16d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
